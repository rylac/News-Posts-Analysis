{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing posts on \"Reddit like\" website Hacker News - Ryan Lacarne\n",
    "based on Hacker News posts from September 2015-2016.\n",
    "\n",
    "In this project, I analyze different posts to see what trends I can spot between posts and the number of comments, ratings, and attention they receive on the website \"Hacker News\".\n",
    "In this project I used the datetime module quite extensively, as well was the random module for the first time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, I will read in the data and get an overview of the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'],\n",
       " ['12579008',\n",
       "  'You have two days to comment if you want stem cells to be classified as your own',\n",
       "  'http://www.regulations.gov/document?D=FDA-2015-D-3719-0018',\n",
       "  '1',\n",
       "  '0',\n",
       "  'altstar',\n",
       "  '9/26/2016 3:26'],\n",
       " ['12579005',\n",
       "  'SQLAR  the SQLite Archiver',\n",
       "  'https://www.sqlite.org/sqlar/doc/trunk/README.md',\n",
       "  '1',\n",
       "  '0',\n",
       "  'blacksqr',\n",
       "  '9/26/2016 3:24'],\n",
       " ['12578997',\n",
       "  'What if we just printed a flatscreen television on the side of our boxes?',\n",
       "  'https://medium.com/vanmoof/our-secrets-out-f21c1f03fdc8#.ietxmez43',\n",
       "  '1',\n",
       "  '0',\n",
       "  'pavel_lishin',\n",
       "  '9/26/2016 3:19'],\n",
       " ['12578989',\n",
       "  'algorithmic music',\n",
       "  'http://cacm.acm.org/magazines/2011/7/109891-algorithmic-composition/fulltext',\n",
       "  '1',\n",
       "  '0',\n",
       "  'poindontcare',\n",
       "  '9/26/2016 3:16']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from csv import reader\n",
    "opened_file = open('hacker_news.csv',encoding=\"utf8\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "hn[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, as customary, I remove the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the dataset is 293119 rows.\n"
     ]
    }
   ],
   "source": [
    "hn_header = (hn[0])\n",
    "hn = (hn[1:])\n",
    "(hn[:5])\n",
    "print (\"The length of the dataset is\", len(hn), \"rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Wrangling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better sample, I will now remove all the submissions that did not receive any comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80401\n",
      "212718\n"
     ]
    }
   ],
   "source": [
    "hn_commented = []\n",
    "hn_nocomments = []\n",
    "for row in hn:\n",
    "    comments = row[4]\n",
    "    if comments != \"0\":\n",
    "        hn_commented.append(row)\n",
    "    else:\n",
    "        hn_nocomments.append(row)\n",
    "\n",
    "hn_commented_len = len(hn_commented)\n",
    "hn_nocomments_len = len(hn_nocomments)\n",
    "print (hn_commented_len)\n",
    "print (hn_nocomments_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is still a heck of a lot of data to work from. I have decided that I would like to work from 20,000 articles. Therefore, I import the random library to create a random sample from my dataset (which now has no articles with 0 comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "hn_final = random.sample(hn_commented,20000)\n",
    "hn_final_len = len(hn_final)\n",
    "print (hn_final_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have successfully done this. The problem is, every time I run the entire code, I actually import a new random set of 20000 articles from the set of 212718 articles with no comments. In the future I would like to revisit this project and fix this so this does not happen, as this is obviously very problematic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two types of posts we'll explore begin with either Ask HN or Show HN. Users submit Ask HN posts to ask the Hacker News community a specific question, and users submit Show HN posts to show the Hacker News community a project, product, or just generally something interesting.\n",
    "\n",
    "First, let's divide these posts up out of our sample of 20000 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1728 Ask HN posts.\n",
      "There are 1324 Show HN posts.\n",
      "There are 16948 posts that are neither Ask HN or Show HN.\n"
     ]
    }
   ],
   "source": [
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "for row in hn_final:\n",
    "    title = row [1]\n",
    "    title = title.lower()\n",
    "    if title.startswith(\"ask hn\"):\n",
    "        ask_posts.append(row)\n",
    "    elif title.startswith(\"show hn\"):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "ask_posts_len = len(ask_posts)\n",
    "show_posts_len= len(show_posts)\n",
    "other_posts_len = len(other_posts)\n",
    "\n",
    "print (\"There are\" ,ask_posts_len, \"Ask HN posts.\")\n",
    "print (\"There are\" ,show_posts_len ,\"Show HN posts.\")\n",
    "print (\"There are\" ,other_posts_len ,\"posts that are neither Ask HN or Show HN.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the Attributes of the Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments on Ask HN posts is  14.0 .\n"
     ]
    }
   ],
   "source": [
    "total_ask_comments = 0\n",
    "for i in ask_posts:\n",
    "    total_ask_comments += float(i[4])\n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "print (\"The average number of comments on Ask HN posts is \",round(avg_ask_comments,0),\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of comments on Show HN posts is  18.0 .\n"
     ]
    }
   ],
   "source": [
    "total_show_comments = 0\n",
    "for i in show_posts:\n",
    "    total_show_comments += float(i[4])\n",
    "avg_show_comments = total_ask_comments / len(show_posts)\n",
    "print (\"The average number of comments on Show HN posts is \",round(avg_show_comments,0),\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is a higher average number of \"Show HN\" posts, I will focus on Show HN for the rest of these projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to try to determine if ask posts created at a certain time are more likely to attract comments. To do so, I will:\n",
    "1. Calculate the amount of show posts created in each hour of the day, along with the number of comments received.\n",
    "2. Calculate the average number of comments show posts receive by hour created.\n",
    "\n",
    "For this, I use the datetime module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up, I calculate the amount of show posts created in each hour of the day, along with the number of comments received.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'04': 357.0,\n",
       " '20': 629.0,\n",
       " '19': 995.0,\n",
       " '21': 345.0,\n",
       " '18': 1039.0,\n",
       " '12': 1215.0,\n",
       " '17': 762.0,\n",
       " '05': 329.0,\n",
       " '22': 343.0,\n",
       " '16': 970.0,\n",
       " '14': 967.0,\n",
       " '15': 1024.0,\n",
       " '07': 302.0,\n",
       " '02': 443.0,\n",
       " '13': 650.0,\n",
       " '09': 374.0,\n",
       " '23': 599.0,\n",
       " '10': 382.0,\n",
       " '11': 764.0,\n",
       " '00': 338.0,\n",
       " '01': 310.0,\n",
       " '08': 440.0,\n",
       " '06': 307.0,\n",
       " '03': 221.0}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "result_list = []\n",
    "for row in show_posts:\n",
    "    creationtime = row[6]\n",
    "    comments = float(row [4])\n",
    "    result_list.append([creationtime,comments])\n",
    "\n",
    "    \n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "for row in result_list:\n",
    "    date = row[0]\n",
    "    comment = row[1]\n",
    "    time = dt.datetime.strptime(date, date_format).strftime(\"%H\")\n",
    "    if time in counts_by_hour:\n",
    "        comments_by_hour[time]+= comment\n",
    "        counts_by_hour[time] += 1\n",
    "    else: \n",
    "        comments_by_hour[time] = comment\n",
    "        counts_by_hour[time] = 1\n",
    "        \n",
    "comments_by_hour\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next , I calculate the average number of comments show posts receive by hour created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['04', 17.0],\n",
       " ['20', 9.530303030303031],\n",
       " ['19', 12.922077922077921],\n",
       " ['21', 6.2727272727272725],\n",
       " ['18', 12.670731707317072],\n",
       " ['12', 13.651685393258427],\n",
       " ['17', 7.855670103092783],\n",
       " ['05', 11.344827586206897],\n",
       " ['22', 6.125],\n",
       " ['16', 8.899082568807339],\n",
       " ['14', 11.114942528735632],\n",
       " ['15', 11.636363636363637],\n",
       " ['07', 10.413793103448276],\n",
       " ['02', 15.275862068965518],\n",
       " ['13', 8.125],\n",
       " ['09', 7.333333333333333],\n",
       " ['23', 17.114285714285714],\n",
       " ['10', 8.681818181818182],\n",
       " ['11', 13.89090909090909],\n",
       " ['00', 9.135135135135135],\n",
       " ['01', 12.4],\n",
       " ['08', 12.571428571428571],\n",
       " ['06', 10.964285714285714],\n",
       " ['03', 11.05]]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_by_hour = []\n",
    "\n",
    "for hr in comments_by_hour:\n",
    "    avg_by_hour.append([hr,comments_by_hour[hr]/counts_by_hour[hr]])\n",
    "    \n",
    "avg_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives me the hour, followed by the average number of posts received in that hour, but this list of list is pretty horrendous to read, therefore I use the sorted function to return the hours with the most comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[17.114285714285714, '23'],\n",
       " [17.0, '04'],\n",
       " [15.275862068965518, '02'],\n",
       " [13.89090909090909, '11'],\n",
       " [13.651685393258427, '12'],\n",
       " [12.922077922077921, '19'],\n",
       " [12.670731707317072, '18'],\n",
       " [12.571428571428571, '08'],\n",
       " [12.4, '01'],\n",
       " [11.636363636363637, '15'],\n",
       " [11.344827586206897, '05'],\n",
       " [11.114942528735632, '14'],\n",
       " [11.05, '03'],\n",
       " [10.964285714285714, '06'],\n",
       " [10.413793103448276, '07'],\n",
       " [9.530303030303031, '20'],\n",
       " [9.135135135135135, '00'],\n",
       " [8.899082568807339, '16'],\n",
       " [8.681818181818182, '10'],\n",
       " [8.125, '13'],\n",
       " [7.855670103092783, '17'],\n",
       " [7.333333333333333, '09'],\n",
       " [6.2727272727272725, '21'],\n",
       " [6.125, '22']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap_avg_by_hour = []\n",
    "\n",
    "for row in avg_by_hour:\n",
    "    swap_avg_by_hour.append([row[1],row[0]])\n",
    "\n",
    "sorted_swap = sorted(swap_avg_by_hour, reverse=True)\n",
    "\n",
    "sorted_swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I print the Top 5 \"Prime Time\" Hours for Show HN posts, where there were the most comments, in a readable and easy to undersatnd format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours for 'Show HN' Comments\n",
      "23:00: 17.11 average comments per post\n",
      "04:00: 17.00 average comments per post\n",
      "02:00: 15.28 average comments per post\n",
      "11:00: 13.89 average comments per post\n",
      "12:00: 13.65 average comments per post\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Hours for 'Show HN' Comments\")\n",
    "for avg, hr in sorted_swap[:5]:\n",
    "    print(\n",
    "        \"{}: {:.2f} average comments per post\".format(\n",
    "            dt.datetime.strptime(hr, \"%H\").strftime(\"%H:%M\"),avg\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The biggest challenge in this project was definitely getting used to the rather complex syntax of the datetime module, as well as trying to figure out how to use the random module in a way that doesn't hange the dataset I'm working with every time I run this program in Jupyter. I hope to be able to revisit this project in the future and fix these things. Again, just like my first project, I believe I would definitely be able to benefit from using numpy and pandas when working with the csv, to save both programmer and machine time, but this will come in future projects. \n",
    "\n",
    "Beyond that, I think it's pretty cool that I managed to figure out the \"Prime Time\" of the Hacker News website for a specific type of post. This type of information is definitely something that would be useful in a business and more specifically marketing setting, where a company may want to figure out at what time their audience is most active on a particular platform, when to launch a certain product, discussion, etc. I look forward to using this kind of methodology again."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
